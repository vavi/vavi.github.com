---
layout: post
title: "分布式存储系统概述"
description: ""
category: 分布式系统
tags: [分布式系统]
---
 
## 基础概念
### 网络调用3态
网络RPC调用3种结果：成功，失败，超时。一旦发生超时后，无法确认是否调用成功。
### CAP 
CAP是指
一致性：读操作总是能够读取之前完成的写操作结果，满足这个条件我们称之为强一致性系统
可用性：读写操作单台机器发生故障的情况下仍然能够正常执行，不需要等待发生故障的机器重启或者该机器上面的服务前移到其他机器
分区可容忍性：机器故障，网络故障，机房停电等异常情况下仍然满足一致性和可用性。

CAP 理论的定义很简单，CAP 三个字母分别代表了分布式系统中三个相互矛盾的属性： 
Consistency (一致性)：CAP 理论中的副本一致性特指强一致性（1.3.4  ）；  
Availiablity(可用性)：指系统在出现异常时已经可以提供服务； 
Tolerance to the partition of network (分区容忍)：指系统可以对网络分区（1.1.4.2  ）这种异常情
况进行容错处理； 
CAP 理论指出：无法设计一种分布式协议，使得同时完全具备 CAP 三个属性，即 1)该种协议
下的副本始终是强一致性，2)服务始终是可用的，3)协议可以容忍任何网络分区异常；分布式系统
协议只能在 CAP 这三者间所有折中。 




CAP 理论指出：无法设计一种分布式协议，使得同时完全具备 CAP 三个属性，即 1)该种协议
下的副本始终是强一致性，2)服务始终是可用的，3)协议可以容忍任何网络分区异常；分布式系统
协议只能在 CAP 这三者间所有折中。 

C 所有节点看到一致的数据 ，3个副本由2个写成功即可。 All or Nothing，or partcial???
 

虽然不是一个层面上的概念，但是这里也要把ACID拿出来复习下。
ACID，Atomic，Consistent，Isolated，Durable

ACID的C不同于CAP中的C
http://zh.wikipedia.org/wiki/ACID

原子性，保证操作的逻辑，要么最终全部成功，要么最终全部失败。一致性约束可见性，要么看到的是事务处理前的状态，要么是处理完的状态，处理中的状态不可见。持久性就是事务结束后看到的状态，不可退回事务前的状态。 隔离性，与性能有关，影响到一致性保证的级别。 隔离性的概念一知半解，呵呵

原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。
一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的默认规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。
隔离性：当两个或者多个事务并发访问（此处访问指查询和修改的操作）数据库的同一数据时所表现出的相互关系。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。
持久性：在事务完成以后，该事务对数据库所作的更改便持久地保存在数据库之中，并且是完全的。

ACID 一致性  两个事务，一增一减  i 4个隔离级别 RU RC RR SR

base

TCC

### 副本一致性
副本指在分布式系统中为数据或者服务提供的冗余。副本一致性是针对分布式系统而言的。

备份和低一致性 来提升可用性和网络分区容忍性

强一致性 单调一致性  会话一致性  最终一致性 弱一致性 
节选自
强一致性(strong consistency)：任何时刻任何用户或节点都可以读到最近一次成功更新的副本数
据。强一致性是程度最高的一致性要求，也是实践中最难以实现的一致性。 
单调一致性(monotonic consistency)：任何时刻，任何用户一旦读到某个数据在某次更新后的值，
这个用户不会再读到比这个值更旧的值。单调一致性是弱于强一致性却非常实用的一种一致性级别。
因为通常来说，用户只关心从己方视角观察到的一致性，而不会关注其他用户的一致性情况。 
会话一致性(session consistency)：任何用户在某一次会话内一旦读到某个数据在某次更新后的值，
这个用户在这次会话过程中不会再读到比这个值更旧的值。会话一致性通过引入会话的概念，在单
调一致性的基础上进一步放松约束，会话一致性只保证单个用户单次会话内数据的单调修改，对于
不同用户间的一致性和同一用户不同会话间的一致性没有保障。实践中有许多机制正好对应会话的
概念，例如 php 中的 session 概念。可以将数据版本号等信息保存在 session 中，读取数据时验证副
本的版本号，只读取版本号大于等于 session 中版本号的副本，从而实现会话一致性。 
最终一致性(eventual consistency)：最终一致性要求一旦更新成功，各个副本上的数据最终将达
到完全一致的状态，但达到完全一致状态所需要的时间不能保障。对于最终一致性系统而言，一个
用户只要始终读取某一个副本的数据，则可以实现类似单调一致性的效果，但一旦用户更换读取的
副本，则无法保障任何一致性。 
弱一致性(week  consistency)：一旦某个更新成功，用户无法在一个确定时间内读到这次更新的
值，且即使在某个副本上读到了新的值，也不能保证在其他副本上可以读到新的值。弱一致性系统
一般很难在实际中使用，使用弱一致性系统需要应用方做更多的工作从而使得系统可用。 



### 副本控制协议
从上面引出副本一致性问题
 中心化副本控制协议
primary-secondary（也称primary-backup）。在该协议中，副本被分成两大类：primary副本，非primary副本，简称为secondary副本。维护primary副本的节点作为中心节点，中心节点负责维护数据的更新，并发控制和协调副本的一致性。

需要解决数据更新流程，数据读取流程，primary副本的确定和切换，数据同步。

* 外部节点把更新操作发给primary节点
* primary节点确定并发更新操作的先后顺序
* primary节点将更新操作发送给secondary节点
* primary节点根据secondary节点的完成情况决定更新操作是否成功，并将结果返回给外部节点。

如果由primary直接同时想其他N个副本的发送数据的话，那么每个secondar的更新吞吐受限于primary出口带宽的限制。所以，有些系统会使用副本接力的方式，比如primary发送个第一个secondary副本数据，第一个secondary副本发送第二个secondary数据，依次类推。

数据读取 由两种思路，随机确定primary副本，将数据分为数据段，以数据段为副本的基本单位，将副本分散到集群中。只要primary副本分散到集群中，即使只有primary副本提供读写服务，也可以充分利用集群机器资源。另一中思路就是主副本记录secondary是否可用。如果不可用，那么secondary副本可以继续尝试与primary副本同步数据，当完成数据同步时，primary副本将其标记位可用。

primary副本需要元数据服务器去维护。 存在如下问题，如何确定节点的状态以及发现原primary节点异常；切换primary后，不能影响副本一致性。 缺陷是，由于primary切换带来一定的停服时间。

数据同步存在3种情况：备副本完全落后主副本数据（新增副本节点），部分落后主副本数据（网络分化等原因），存在冗余副本数据（主副本没有更新，备副本反而进行了多余的修改操作）。3种解决方法 ：复制快照数据， 回放redo log，基于undo log 删除多余数据

副本复制分为两种，强同步复制和异步复制，两者的区别在于用户的请求是否在数据复制到其他副本上才返回成功。

一致性和可用性是矛盾的。强同步的问题的当备副本出现故障时，可能阻塞存储系统的正常写服务，可用性较差。异步复制协议的可用性较好，但是一致性得不到保障，主副本出现故障是还有数据丢失的可能。使用qurom 英 [ˈkwɔ:rəm]，法定人数。

复制概述：常见的做法是同步操作日志。主副本首先将操作日志同步到备副本，备副本回放操作日志，完成后通知主副本。接着，主副本修改本机，等到所有的操作都完成后再通知客户端写成功。

预写式日志（Write-ahead logging，缩写 WAL）通常包括redo和undo信息。根据数据重要级别，决定是否每次都将数据落盘。

checkpoint 系统定期将内存的操作以某种易于加载的形式（checkpoint文件）转存到磁盘中，并记录checkpoint时刻的日志回放点，以后故障恢复只需要回放checkpoint时刻之后的redo日志。


### 数据分布方式

hash 用户id的hash值 mod 服务器个数，缺点是扩展性差。一旦服务器个数变化，几乎所有数据需要被迁移并重新分布。可以增加中间层，即元数据服务器。需要防止数据倾斜问题。

还有两种，按数据范围分布和按数据量分布 。缺点都是元数据管理复杂。

一致性hash 通过引入虚节点的概念，虚节点的个数一般远大于未来集群中的机器数。首先根据数据的hash值找到虚节点，然后从元数据中找到真实节点。无论是增加节点还是节点宕机，都有助于实现全局负载均衡。

在存在多个副本的情况下，一般数据是以块的方式而不是机器的方式存放数据。这样便于在节点宕机后快速恢复数据以及负载均衡。

移动数据不如移动计算，本地化计算。

分库分表

 
数据分裂和合并。

----

## 分布式原理
### 一致性哈希 

在常见的分布式缓存系统中，有N台服务器提供缓存服务，需要对服务器进行负载均衡，将请求平均分发到每台服务器上，每台机器负责1/N的服务。

常用的简单算法是

1. 对机器从0到N-1进行编号
2. 计算key的hashcode，然后对hashcode进行求余计算：i=key.hashcode mod N
3. 然后客户端请求分发到编号为i的机器


但这样的算法方法存在严重问题：当服务器宕机或者增加新的机器后，大量缓存需要重新计算，分配到新的服务器上。这样会导致在缓存迁移过程中，缓存大量失效，请求压力会大量透传到后端业务系统中。可能导致业务系统不稳定。

一致哈希算法来避免这样的问题。 一致哈希尽可能使同一个资源映射到同一台缓存服务器。这种方式要求增加一台缓存服务器时，新的服务器尽量分担存储其他所有服务器的缓存资源。减少一台缓存服务器时，其他所有服务器也可以尽量分担存储它的缓存资源。 

一致哈希算法的主要思想是将每个缓存服务器与一个或多个哈希值域区间关联起来，其中区间边界通过计算缓存服务器对应的哈希值来决定。如果一个缓存服务器被移除，则它会从对应的区间会被并入到邻近的区间，其他的缓存服务器不需要任何改变。

一致哈希算法的过程是：

1. 求出每个服务节点的hash，并将其配置到一个0~2^32的圆环（continuum）区间上。
2. 使用同样的方法求出你所需要存储的key的hash，也将其配置到这个圆环（continuum）上。
3. 从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务节点上。如果超过2^32仍然找不到服务节点，就会保存到第一个服务节点上。


虚拟节点(virtual nodes)： 之所以要引进虚拟节点是因为在服务器较少的情况下，通过hash(key)算出节点的哈希值在圆环上并不是均匀分布的(稀疏的)，仍然会出现各节点负载不均衡的问题。比如说只有3台服务器，当加入一个后，在NODE0 和 NODE2中间加入一个NODE3节点；NODE0的原先的请求压力被NODE3分摊了一半，但是NODE1和NODE2的压力一点变化都没有。

为了解决圆环上节点不均匀的问题，可以将每台物理机对应一组虚拟机器。将虚拟机器的hash值放置到圆环上，key在
圆环上先找到虚拟机器节点，然后根据虚拟机器节点找到物理机器节点。

虚拟节点可以认为是实际节点的复制品(replicas)，本质上与实际节点实际上是一样的(key并不相同)。引入虚拟节点后，通过将每个实际的服务器(节点)数按照一定的比例(例如200倍)扩大后并计算其hash(key)值以均匀分布到圆环上。
 

### 2阶段提交

两阶段提交：该协议通常用来实现分布式事务。系统一般包含两类节点：一类是协调者（coordinator），通常一个系统只有一个；另一类是事务参与者（participants），一般包含多个。协议中假设每个节点都会记录操作日志并持久化到非易失性存储介质。

它分成两个阶段，称之为准备阶段，提交阶段；也可以理解为投票，确认过程:

1. 准备阶段（Prepare Phrase）：
	* 协调者会问所有的参与者结点，是否可以执行提交操作。
	* 各个参与者开始事务执行的准备工作：如：为资源上锁，预留资源，写undo/redo log……。
	* 参与者响应协调者，如果事务的准备工作成功，则回应“可以提交”，否则回应“拒绝提交”。
2. 确认阶段（Confirm Phrase）：
    * 如果所有的参与者都回应“可以提交”，那么，协调者向所有的参与者发送“正式提交”的命令。
    * 参与者完成正式提交，并释放所有资源，然后回应“完成”，
    * 协调者收集各结点的“完成”回应后结束这个Global Transaction。
	* 如果有一个参与者回应“拒绝提交”，那么，协调者向所有的参与者发送“回滚操作”，并释放所有资源.
	* 参与者然后回应“回滚完成”，协调者收集各结点的“回滚”回应后，取消这个Global Transaction。


异常流程的话，可以参考酷壳的这篇文章[分布式系统的事务处理](http://coolshell.cn/articles/10910.html)以及《分布式系统原理介绍》电子书的相应章节介绍。

### PAXOS

多个节点直接通过操作日志同步数据，如果只有一个节点称为主节点，就很容易在多个节点之间维护数据一致性。然后主节点可能出现故障，那么就需要选出主节点。Paxos协议就是用于解决多个节点之间的一致性问题。

PAXOS协议的角色分为 proposers，acceptors，和 learners（允许一个节点身兼数职）。proposers 提出提案，提案信息包括提案编号和提议的value；acceptor 收到提案后可以接受（accept）提案，若提案获得多数 acceptors 的接受，则称该提案被批准（chosen）；learners 只能「学习」被批准的提案。

通过一个决议分为两个阶段：

1. 准备阶段：
	* proposer选择一个提案序号n并将prepare 请求发送给其他acceptors ；
	* acceptor收到prepare消息后，如果提案的编号大于它已经回复的所有prepare消息，则acceptor将自己上次接受的提案回复给proposer，并承诺不再回复小于n的提案；
2. 批准阶段：
	* 当一个proposer收到了多数acceptors对prepare的回复后，就进入批准阶段。
	* 如果在之前的prepare阶段accptor回复了上次接受的提议，那么，proposer选择其中序号最大的提议值发送个acceptor批准
	* Acceptor在不违背自己向其他proposer的承诺的前提下，acceptor在收到accept请求后即接受这个请求。

在上述prepare过程中，如果一个acceptor发现存在一个更高编号的提案，则需要通知proposer，提醒其中断这次提案。

### 其他

* Lease:中文叫租约，通常定义为：颁发者在一定期限内給予持有者一定权利的协议。Lease 表达了颁发者在一定期限内的承诺，只要未过期颁发者必须严格遵守lease约定的承诺。Lease 的持有者在期限内使用颁发者的承诺，但lease一旦过期必须放弃使用或者重新和颁发者续约。
打个比方，Lease就是租房时候的承诺。要续租的话，提前一个星期把房租打给房东。房东答应你在你租房的这段时间内，房子不会被别人适用。同时，你也不需要频繁的问房东，房子是否现在只有我一个人使用。详细介绍见[Lease 机制在分布式系统中的应用](http://blog.csdn.net/mindfloating/article/details/7903219)

* MVCC:Multiversion concurrency control，多版本并发控制。本质是使用COW+Version+CAS这个技术组合来提升系统并发性。详细介绍见[多版本并发控制(MVCC)在分布式系统中的应用](http://coolshell.cn/articles/6790.html)
 
---

## 分布式存储系统

分布式存储系统通常需要解决如下一些问题
* 数据分布  在多台服务器之间保证数据分布均匀，跨服务器如何读写
* 一致性  异常情况下如何保证副本一致性
* 容错  把发生故障当成常态来设计，做到检测是否发生故障并进行故障迁移
* 负载均衡 新增、移除服务器时如何负载均衡 数据迁移如何不影响已有服务
* 事务并发控制  如何实现分布式事务，如何实现多版本并发控制
* 压缩、解压缩 根据数据特点选择恰当算法，如何平衡时间和空间的关系。

数据分类：

* 非结构化数据：文本，图片，视频音频
* 结构化数据：数据模式（属性，数据类型，数据之间的联系）与内容是分开定义。
* 半结构化数据：模式和内容混在一起，没有明显的区分。
 

行式存储 
列式存储



hostclass —》 host
 at least once，重发+唯一主键策略。
故障检测，Faliure  Detection

数据压缩 批处理

索引，二级索引 

IOPS，即I/O per second，即每秒读写（I/O）操作的次数  http://zh.wikipedia.org/wiki/%E4%BA%8B%E5%8A%A1%E5%86%85%E5%AD%98

事务内存目前有两种实现方式，基于软件的STM（Software Transactional Memory）和基于硬件的HTM（Hardware Transactional Memory）。 
 

单机 
多机
网络拓扑
协议定义
协议实现
google CLOS

log（事务id+动作+状态），然后基于log去推理，计算。对于单机事务可以基于wal（redo、undo）来恢复一致性。粗粒度的可以使用TCC服务。

元数据可以找服务器确定，也可以基于算法去确定。当主机宕机后，存在某个算法的下一个主机。当主机恢复启动时，可以问下我是否错过什么数据。如果错太远，错一点， 注意负载均衡 和网络带宽，磁盘io等。 主机节点的唯一性。

分布式存储的问题，尝试解答。
 
 
 redo undo 定期刷新
存储 分为 大文件，小文件，结构化，非结构化，分布式文件存储和数据库存储。数据库存储 读写分离，垂直扩展（scale up） 分表，水平扩展（scale out）分库， 

TDDL ， sql proxy、agent，binlog sync 分区分表 

存储    db,nosql,hbase, 分区表，字段压缩，查询优化，awr报告。

连接泄露原理是查询session视图，获得每个session的创建时间，看看那些session生命周期过长。然后找到对应执行的sql语句和业务代码。

略过，找到重要细节进行总结。从功能，适用场景，易用性，不断完善，

#### 分布式文件系统
适合存储blob对象。
一个大文件分成多个数据块（chunk），每个数据块大小相当。分布式文件系统将这些数据块分散到存储机器，并将用户对数据的操作转化为对若干个数据块的操作。

一个面向大规模分布式数据密集性应用的可
扩展分布式文件系统。它运行在廉价的商品化硬件上提供容错功能，为大量的客
户端提供高的整体性能。 

大部分的文件更新模式是通过在尾部追加数据而不是覆盖现有数据。文件内 部的随机写操作几乎是不存在的。

文件被划分成固定大小的chunk。每个 chunk是由chunk创建时由master分配的 一个不可变的全局唯一的 64bit 句柄来标识。Chunkserver 将 chunk 作为 linux 文 件存储在本地,对于 chunk 数据的读写通过 chunk 的 handle 和字节边界来表示。 为了可靠性,每个 chunk 存储在多个 chunkserver 上。尽管用户可以为不同文件 名字空间区域指定不同的备份级别,默认地我们存储三个备份。

Master 维护所有的文件系统元数据。包括名字空间,访问控制信息,文件与 chunk 的映射信息,chunk 的当前位置。它也控制系统范围内的一些活动,比如 chunk 租约管理,无效 chunk 的垃圾回收,chunkserver 间的 chunk 迁移。Master 与 chunkserver 通过心跳信息进行周期性的通信,以发送指令和收集 chunkserver 的 状态。
应用程序链接的 GFS 客户端代码实现了文件系统 API,应用程序通过它与 master 和 chunkserver 进行通信以读写数据。客户端如果需要操作元数据则需要与 master 通信,但是所有的纯数据通信直接与 chunksever 通信。
只有一个 master 大大简化了我们的设计,而且使得 master 可以利用全局信息对 chunk 的放置和备份进行更好的判断。然而,我们必须最小化它在读写中的参与 度,使得它不会成为一个瓶颈。Client 永远不会通过 master 读取文件数据,它只是问 master 它应该同哪个 chunkserver 联系。并且 client 会在一定的时间段内缓 存这些信息,直接与 chunksever 交互进行很多后续的操作。
根据图 1,我们解释一下一个简单的读操作的交互过程:首先,由于 chunk 的大 小固定,客户端就可以将应用程序中标识的文件名和 offset 转换为 chunk 的 index。 然后给master发送一个包含文件名和chunk index的请求,master返回相应的 chunk的handle和所有备份的位置。客户端以文件名和chunk index为key将这 条信息进行缓存。然后客户端给其中一个备份发送一个请求,通常是最近的那个。请求标识了 chunk 的 handle 以及在那个 chunk 内的字节边界。直到缓存信息过期或者重新打开文 件之前,对于相同 chunk 的后续读操作就不需要 client-master 的通信了。事实上, 客户端通常在一个请求中查询多个 chunk 的信息,master 也可以将这些被请求的 多个 chunk 的信息包裹在一块进行返回。通过使用这种特别的信息,没有增加额 外的花费就避免了未来 client-master 间的多次通信。
chunk 大小是一个关键的设计参数。我们选择了 64MB。首先,降低了 client 与 master 的交互需 求,因为在相同 chunk 上的读写只需要一个初始化请求就可以从 master 得到 chunk 的位置信息。这对于减少应用产生的负载是非常明显的,因为大部分应用 需要顺序的读写整个大文件。即使对于小的随机读取,客户端也可以很容易的缓 存一个几 TB 工作集的所有 chunk 的位置信息。其次,由于 chunk 很大,那么客 户端就很有可能在一个给定的 chunk 上执行更多的操作,这样可以将一个与 chunkserver 的 TCP 连接保持更长的时间,这就减少了网络开销。再者,降低了 存储在 master 上的元数据大小。这样就允许我们将元数据存放在内存中,反过 来就带来了我们将在 2.6.1 中讨论的其他优势。一个潜在的长远解决方案是在这种情况下,允许客户端从其他客户端读取数据，用来解决chunk文件热点问题。
Master 存储了三个主要类型的元数据:文件和 chunk 名字空间,文件到 chunk 的映射信息,每个 chunk 的备份的位置。所有的元数据都保存在 master 的内存 中。前两种类型还通过将更新操作的日志保存在本地硬盘和备份在远程机器来保 持持久化。使用 log 允许我们简单可靠地更新 master 的状态,不用担心当 master crash 时的不一致性。Master 并没有永久保存 chunk 的位置信息,而是在 master 启动或者某个 chunkserver 加入集群时,它会向每个 chunkserver 询问它的 chunks 信息。
由于元数据存储在内存里,master 的操作是很快的。因此对于 master 来说,可 以简单有效地在后台对整个状态进行周期性扫描。这个周期性的扫描是用来实现 chunk 垃圾回收,chunkserver 出现失败时进行的重复制,以及为了平衡负载和磁 盘空间在 chunkserver 间的 chunk 迁移。4.3,4.4 将进一步讨论这些活动。由于元数据存储在内存里,master 的操作是很快的。因此对于 master 来说,可 以简单有效地在后台对整个状态进行周期性扫描。这个周期性的扫描是用来实现 chunk 垃圾回收,chunkserver 出现失败时进行的重复制,以及为了平衡负载和磁 盘空间在 chunkserver 间的 chunk 迁移。4.3,4.4 将进一步讨论这些活动。全内存策略存在的一个潜在限制就是 chunk 的数目,因此整个系统的容量取决于 master 有多少可用内存。实际中这不是一个很严重的限制。Master 为每个 64MB 的 chunk 维护少于 64byte 的数据。大部分的 chunk 是满的,因为大部分的文件 包含多个 chunk,只有最后一个 chunk 可能是未满的。类似的,每个文件名字空 间数据通常需要少于 64byte 因为文件名称存储时会使用前缀压缩算法进行压缩。
操作日志包含了关键元数据改变的历史记录。它是 GFS 的核心。它不仅是元数据 的唯一一致性记录,而且它也定义了那些并发操作的逻辑上的时间表。文件和 chunk 的版本都是唯一和永恒地由它们创建时的逻辑时间来标识的。
检查点保存了一个压缩的类 B 树的结构, 不需要额外的解析就可以直接映射到内存用于名字空间查找。这大大提高了恢复 的速度和可用性。
GFS 应用程序可以通过使用简单的技术来适应这种放松的一致性模型,这些技术 已经为其他目的所需要:依赖于 append 操作而不是覆盖、检查点、写时自我验 证、自标识-记录。
实际中,我们所有的应用程序都是通过 append 而不是覆盖来改变文件。at-least-append。但是偏移位置是 由 GFS 决定的(然而,通常的理解可能是在客户端想写入的那个文件的尾部)。偏 移位置会被返回给客户端,同时标记包含这条记录的那个已定义的(defined)文件 区域的起始位置。另外 GFS 可能会在它们之间插入一些 padding 或者记录的副本。 它们会占据那些被认为是不一致的区域,通常它们比用户数据小的多。如果我们要保证唯一性,可以在应用层增加逻辑。
Master 授权给其中的一个副本一个该 chunk 的租约,我们 把它叫做主副本(primary)。这个主副本为针对该 chunk 的所有的变更选择一个执 行顺序,然后所有的副本根据这个顺序执行变更。因此,全局的变更顺序首先是 由 master 选择的租约授权顺序来确定的(可能有多个 chunk 需要进行修改),而同 一个租约内的变更顺序则是由那个主副本来定义的。
假设client将数据推送给S1- S4,它会首先将数据 推送给最近的 chunkserver 假设是 S1,S1 推送给最近的,假设是 S2,S2 推送给 S3、S4 中离他最近的那个。我们网络拓扑足够简单,以至于距离可以通过 IP 地 址估计出来。
Record append是一种类型的变更操作,依然遵循3.1节的控制流,只是在主副 本上会有一点额外的逻辑。Client 将所有的数据推送给所有副本后,它向主副本 发送请求。主副本检查将该记录 append 到该 chunk 是否会导致该 chunk 超过它 的最大值(64MB)。如果超过了,它就将该 chunk 填充到最大值,告诉次副本做同 样的工作,然后告诉客户端该操作应该在下一个 trunk 上重试。(append 的 Record 大小需要控制在最大 trunk 大小的四分之一以内,这样可以保证最坏情况下的碎 片可以保持在一个可以接受的水平上 )。如果记录没有超过最大尺寸,就按照普 通情况处理,主副本将数据 append 到它的副本上,告诉次副本将数据写在相同 的偏移位置上,最后向 client 返回成功应答。

#### 分布式键值系统
基于主键实现CRUD功能，通常使用一致性hash技术。

#### 分布式表格系统
不仅仅支持基于主键的CURD操作，还支持扫描某个主键范围。一般仅支持单表操作。通常存储半结构化数据，不需要预先定义模式。

#### 分布式数据库
二维表格组织数据，支持sql，多表关联，嵌套子查询等复杂操作，并支持并发数据库事务。

#### 小结
抽象，找到共同点，再看看差异点。找到特别的知识。



### 重要细节
同一个数据中心延时较小，网络一次来回的时间在1毫秒之内。北京，杭州距离1300km，光在信息传输时走折线，假设折现距离是直线距离的1.5倍，那么整个耗时 1300*1.5*2/300000=13毫秒，实测时40毫秒。 

分布式存储系统 p10 性能参数 
亚马逊中文论文 ，中间件ali blog
google系列论文

和状态，每次事务开始，都必须记录日志。日志恢复时，根据交互多方所处的状态，决定是继续完成还是放弃完成。放弃后是否再次重试？ 

存在元数据的主节点记录业务状态 tcc（try，confirm，cancel）

csdn文章

---

## 参考

* 《深入分析Java Web 技术内幕》
* 《大规模分布式存储系统原理解析与架构实战》
* 《大型网站技术架构核心原理与案例分析》
* 《分布式系统原理介绍》
* 《大规模Web服务开发技术》
* 《面向模式的软件架构_第4卷_分布式计算的模式语言》
* 《程序员》 2014 第一期
*  [分布式系统的事务处理](http://coolshell.cn/articles/10910.html)
*  [Paxos算法](http://zh.wikipedia.org/zh/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B)
*  [Explanation of BASE terminology](http://stackoverflow.com/questions/3342497/explanation-of-base-terminology)
*  [Lease 机制在分布式系统中的应用](http://blog.csdn.net/mindfloating/article/details/7903219)
*  [详解Consistent Hashing算法](http://developer.51cto.com/art/201104/254419.htm)
*  [一致性hash算法伪码](http://www.berlinix.com/dev/consistency-hash.php)


{% include JB/setup %}
